{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "from transformers import IntervalStrategy, AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
    "from datasets import Dataset\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from evaluate import load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "path_work = \"/home/conchae/PhageDepo_pdb\"\n",
    "\n",
    "# ******************************************************************\n",
    "# LOAD THE DATA :\n",
    "df_depo = pd.read_csv(f\"{path_work}/Dpo_domains.phagedepo.0805.final.tsv\" , sep = \"\\t\" , header = 0)\n",
    "\n",
    "df_beta_helix = df_depo[df_depo[\"Fold\"] == \"right-handed beta-helix\"]\n",
    "df_beta_prope = df_depo[df_depo[\"Fold\"] == \"6-bladed beta-propeller\"]\n",
    "\n",
    "def get_labels(df , label = 1) :\n",
    "    labels_df = []\n",
    "    for _,row in df.iterrows():\n",
    "        info = row[\"Boundaries\"]\n",
    "        seq_length = len(row[\"Full_seq\"])\n",
    "        if info == \"full_protein\" :\n",
    "            labels = [label] * seq_length\n",
    "            labels_df.append(labels)\n",
    "        else :\n",
    "            start = int(info.split(\"_\")[-2])\n",
    "            end = int(info.split(\"_\")[-1])\n",
    "            labels = [0 if i < start or i >= end else label for i in range(seq_length)]\n",
    "            labels_df.append(labels)\n",
    "    return labels_df\n",
    "\n",
    "# Beta-helix :\n",
    "labels_beta_helix = get_labels(df_beta_helix , label = 1)\n",
    "seq_beta_helix = df_beta_helix[\"Full_seq\"].to_list()\n",
    "\n",
    "# Beta propeller :\n",
    "labels_beta_propeller = get_labels(df_beta_prope , label = 2)\n",
    "seq_beta_propeller = df_beta_prope[\"Full_seq\"].to_list()\n",
    "\n",
    "# The input data :\n",
    "sequences = seq_beta_helix + seq_beta_propeller\n",
    "labels = labels_beta_helix + labels_beta_propeller\n",
    "\n",
    "\n",
    "# ******************************************************************\n",
    "# DEFINING THE MODEL SIZE : \n",
    "#model_checkpoint = \"facebook/esm2_t6_8M_UR50D\"\n",
    "#model_checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n",
    "model_checkpoint = \"facebook/esm2_t30_150M_UR50D\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "\n",
    "# ******************************************************************\n",
    "# PREPROCESS THE DATA FOR THE MODEL :\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "train_tokenized = tokenizer(train_sequences)\n",
    "test_tokenized = tokenizer(test_sequences)\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)\n",
    "\n",
    "num_labels = 3\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "#metric = load(\"accuracy\")\n",
    "\n",
    "# ******************************************************************\n",
    "# THE SEARCH : \n",
    "\n",
    "logging.basicConfig(filename=f'{path_work}/hyperparameters_tuning.log', level=logging.INFO)\n",
    "\n",
    "class ObjectiveWrapper:\n",
    "    def __init__(self, objective):\n",
    "        self.objective = objective\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        result = self.objective(trial)\n",
    "        # Log the result of each trial\n",
    "        logging.info(f'Trial {trial.number} finished with value: {result} and parameters: {trial.params}.')\n",
    "        return result\n",
    "\n",
    "\n",
    "class MyCallback(TrainerCallback):\n",
    "    def __init__(self, trial: optuna.trial.Trial):\n",
    "        self._trial = trial\n",
    "\n",
    "    def on_evaluate(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, metrics=None, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            self._trial.report(metrics[\"eval_loss\"], step=state.global_step)\n",
    "            if self._trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 8, 16)\n",
    "    learning_rate = trial.suggest_float(\"lr\", 1e-6, 1e-4, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "    args = TrainingArguments(\n",
    "        f\"{model_name}-finetuned-depolymerase\",\n",
    "        evaluation_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=weight_decay,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        logging_dir='./logs',\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[MyCallback(trial)]\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"accuracy\"]\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "wrapper = ObjectiveWrapper(objective)\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(\n",
    "    n_startup_trials=5,\n",
    "    n_warmup_steps=30\n",
    ")\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "study.optimize(wrapper, n_trials=20, n_jobs=5)\n",
    "\n",
    "# Log the result of the optimization\n",
    "best_trial = study.best_trial\n",
    "logging.info(f'Best trial finished with value: {best_trial.value} and parameters: {best_trial.params}.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
