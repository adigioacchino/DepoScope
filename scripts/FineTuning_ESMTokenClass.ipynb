{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Replace <model_checkpoint> with the path to the ESM-2 model checkpoint or the name of the model if you want to use a pre-trained one.\n",
    "model_checkpoint = \"<model_checkpoint>\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace <path_to_dataset> with the path to your dataset file.\n",
    "dataset = pd.read_csv(\"<path_to_dataset>\")\n",
    "# Rename the column containing the labels to \"labels\".\n",
    "dataset = dataset.rename(columns={\"<column_name>\": \"labels\"})\n",
    "\n",
    "# Tokenize the input data.\n",
    "tokenized_inputs = tokenizer(\n",
    "    list(dataset[\"input_text\"]),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Convert the labels to a list of integers.\n",
    "labels = list(dataset[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_inputs,\n",
    "    train_labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace <path_to_validation_set> with the path to your validation set file.\n",
    "validation_set = pd.read_csv(\"<path_to_validation_set>\")\n",
    "validation_set = validation_set.rename(columns={\"<column_name>\": \"labels\"})\n",
    "\n",
    "# Tokenize the validation set inputs.\n",
    "tokenized_validation_inputs = tokenizer(\n",
    "    list(validation_set[\"input_text\"]),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Convert the validation set labels to a list of integers.\n",
    "validation_labels = list(validation_set[\"labels\"])\n",
    "\n",
    "# Evaluate the model on the validation set.\n",
    "eval_results = trainer.evaluate(tokenized_validation_inputs, validation_labels)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace <path_to_saved_model> with the path where you want to save the fine-tuned model.\n",
    "trainer.save_model(\"<path_to_saved_model>\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
