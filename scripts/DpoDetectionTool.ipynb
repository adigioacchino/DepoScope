{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dpo Detection Tool script :\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/concha-eloko/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at /media/concha-eloko/Linux/depolymerase_building/esm2_t12_35M_UR50D-finetuned-depolymerase/checkpoint-198/ were not used when initializing EsmForTokenClassification: ['esm.contact_head.regression.weight', 'esm.contact_head.regression.bias']\n",
      "- This IS expected if you are initializing EsmForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "from collections import Counter\n",
    "import torch\n",
    "path_work = \"/media/concha-eloko/Linux/depolymerase_building\"\n",
    "#path_work = \"/home/conchae/PhageDepo_pdb\"\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model_path = f\"{path_work}/esm2_t12_35M_UR50D-finetuned-depolymerase/checkpoint-198/\"\n",
    "#model_path = \"/home/conchae/PhageDepo_pdb/script_files/esm2_t30_150M_UR50D-finetuned-depolymerase/checkpoint-198\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode input text\n",
    "input_text = \"MTTYKTGNPLGSAAVKDLFDNAENLDHFENDRSNETWENRFGVPGKTRYGMEQEHDRQISSQEARFQQFLLSSGYVFLGDYQDGPFQFGARNQYIRYDNQYYRLNAATDVGFTTTGTDATSFANDVTHFVLMDGDTLRQNLSSGDGFKWVGQVSSAAALSALPGSEGDRVLLLGYQDGWAATNSDLSGGGEFHYVSSLASVNNGVTVFNGWVRKFTRSVITTFDAGLSPGEDTDHFPQIQKLLEVVPDGFTVEIHGEHRLSSQLIMEVKRNITIVGVAAKLTTKPYKSTIKVVRLASDGITYMGGILSAINCPGLRLDGDLEIEGTRMYPNVLRSDQTAAGEEHGLHFRYCDDLYIGKDIYVHDVFGYGALGVYCNRAFAYRSMLTDTVRESGLNLFGGSVGGRAVGVRTRRTALYGVEIEDTWYGGARDIKVTNCDVEDAFWGIPTINNCSDVEVSGCNIRRARFGAQALQSSAGAYDTRNIRYHDNTYTGCPVGFRTAHPRNVRITRENIDQSEVMPYGYTYPFNNLLFVDNTDRRIFWGPTSSRFLTMVGQTIYIDDVAYTITAAATDATKTGYWKDFATDPDSLVKVTLDKVLPENTDIQTVKSKDWGTAVRGMLTEGRSVNLTIWNNDLTGDSLDSIGIYHNSYNMDGVNTVNESIRGNTFRAHGIWLRMNDAVNTRDVSDNKYADGSQIGISAANLTAAVLSQIKMGNNIRVALPARTSVAAGAVTKYFHANQRYWAVGLRISFTGLSGTGEMRVAIDGTQTHSATSYSTGTAVVEIYGVATFTKGNHQIAINTANSDIVFTSCDIELLIP\"  # Replace this with your input text\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt', truncation= True)\n",
    "\n",
    "# Get token classifications\n",
    "with torch.no_grad() :\n",
    "    outputs = model(input_ids)\n",
    "    \n",
    "logits = outputs.logits\n",
    "probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "labels = model.config.id2label\n",
    "tokens = []\n",
    "token_probs, token_ids = torch.max(probs, dim=-1)\n",
    "#for token_id, token_probs in zip(input_ids[0], probs[0]):\n",
    "#    top_label_id = token_probs.argmax().item()\n",
    "#    tokens.append(int(labels[top_label_id].split(\"_\")[1]))\n",
    "    \n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1605, -0.2644, -0.5807],\n",
       "         [ 1.4988, -0.5955, -0.8199],\n",
       "         [ 1.8725, -0.6285, -0.9200],\n",
       "         ...,\n",
       "         [ 1.7728, -0.5479, -1.0512],\n",
       "         [ 1.6444, -0.4632, -1.0848],\n",
       "         [ 1.4983, -0.4026, -0.7848]]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pad_or_truncate(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([819, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def pad_or_truncate(self, tensor, max_len):\n",
    "    if tensor.size(1) < max_len:\n",
    "        tensor = F.pad(tensor, (0, 0, 0, max_len - tensor.size(1)))\n",
    "    elif tensor.size(1) > max_len:\n",
    "        tensor = tensor[:, :max_len, :]\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_prediction(fasta_txt) :\n",
    "    if len(fasta_txt) > 200 :\n",
    "        input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "        outputs = model(input_ids)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        labels = model.config.id2label\n",
    "        tokens = []\n",
    "        for token_id, token_probs in zip(input_ids[0], probs[0]):\n",
    "            top_label_id = token_probs.argmax().item()\n",
    "            tokens.append(labels[top_label_id].split(\"_\")[1])\n",
    "        return tokens\n",
    "\n",
    "for multifasta in os.listdir(path_fasta) :\n",
    "    if multifasta[-5:] == \"fasta\" :\n",
    "        fastas = SeqIO.parse(f\"{path_fasta}/{multifasta}\" , \"fasta\")\n",
    "        for record in fastas :\n",
    "            locus_tag = record.description.split(\"locus_tag=\")[1].split(\"]\")[0]\n",
    "            annotation = \"_\".join(record.description.split(\"protein=\")[1].split(\"]\")[0].split())\n",
    "            sequence = record.seq\n",
    "            results = make_prediction(sequence)\n",
    "            if \"1\" in dict(Counter(tokens)) :\n",
    "                n_depo = dict(Counter(tokens))[\"1\"]\n",
    "            elif \"2\" in dict(Counter(tokens)) :\n",
    "                n_depo = dict(Counter(tokens))[\"1\"]\n",
    "            else : \n",
    "                n_depo = 0\n",
    "            with open(f\"{path_bea_out}/{locus_tag}.{annotation}__{n_depo}.out\" , \"w\") as outfile :\n",
    "                outfile.write(str(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "tokens = np.array(tokens)  # convert your list to numpy array for convenience\n",
    "\n",
    "# create your plot\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# plot data\n",
    "for i in range(len(tokens) - 1):\n",
    "    if tokens[i] == 0:\n",
    "        color = 'black'\n",
    "    elif tokens[i] == 1:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "        #tokens[i] == 1\n",
    "    plt.plot([i, i+1], [tokens[i], tokens[i+1]], color=color, marker='o')\n",
    "\n",
    "plt.xlabel('Token')\n",
    "plt.ylabel('Label')\n",
    "plt.title('Label for each token')\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.yticks(np.arange(2), ['0', '1'])  \n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dpo_classifier(nn.Module):  # We define a new class Dpo_classifier which inherits from nn.Module. \n",
    "                                  # nn.Module is the base class for all neural network modules in PyTorch.\n",
    "\n",
    "    def __init__(self, pretrained_model, num_classes):  # This is the constructor of the class. \n",
    "                                                         # When we create a new object of this class, \n",
    "                                                         # we have to provide a pretrained_model and the number of classes in the dataset.\n",
    "\n",
    "        super(Dpo_classifier, self).__init__()  # This calls the constructor of the base class nn.Module.\n",
    "\n",
    "        self.max_length = 1024  # Maximum length of the sequence. Any sequence longer than this will be truncated, and any shorter sequence will be padded.\n",
    "\n",
    "        self.pretrained_model = pretrained_model  # We store the pretrained_model in an instance variable.\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size=num_classes, hidden_size=128, num_layers=1, batch_first=True)  # We create an LSTM layer.\n",
    "                                                                                                     # input_size is the number of expected features in the input x.\n",
    "                                                                                                     # hidden_size is the number of features in the hidden state h.\n",
    "                                                                                                     # num_layers is the number of recurrent layers.\n",
    "\n",
    "        self.classifier = nn.Linear(128, 2)  # We create a Linear layer.\n",
    "                                              # This layer multiplies its input with its weight (a matrix) and then adds a bias (a vector).\n",
    "                                              # The output size is 2, corresponding to two classes for binary classification.\n",
    "\n",
    "    def preprocess_sequence(self, fasta_txt):  # Method to preprocess a sequence.\n",
    "        encoded_input = tokenizer(fasta_txt, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')  # We tokenize the sequence.\n",
    "                                                                                                                                    # truncation=True means that the sequence will be truncated to the maximum length if it's longer.\n",
    "                                                                                                                                    # padding='max_length' means that the sequence will be padded to the maximum length if it's shorter.\n",
    "                                                                                                                                    # return_tensors='pt' means that the tokens will be returned as PyTorch tensors.\n",
    "        return encoded_input['input_ids']  # We return only the input_ids. These are the tokenized sequences, where each token has been replaced by its ID in the tokenizer vocabulary.\n",
    "\n",
    "    def pad_or_truncate(self, tensor, max_len):  # Method to pad or truncate a tensor.\n",
    "        if tensor.size(1) < max_len:  # If the second dimension of the tensor (sequence length) is less than max_len,\n",
    "            tensor = F.pad(tensor, (0, 0, 0, max_len - tensor.size(1)))  # we pad the tensor on the second dimension. The padding value is 0.\n",
    "        elif tensor.size(1) > max_len:  # If the second dimension of the tensor is greater than max_len,\n",
    "            tensor = tensor[:, :max_len, :]  # we truncate the tensor on the second dimension.\n",
    "        return tensor  # We return the padded or truncated tensor.\n",
    "\n",
    "    def forward(self, sequences):  # Method to compute the forward pass of the model.\n",
    "        inputs = self.preprocess_sequence(sequences)  # We preprocess the sequences.\n",
    "\n",
    "        logits = self.pretrained_model(inputs).logits  # We pass the preprocessed sequences to the pretrained model and get the logits.\n",
    "\n",
    "        logits = self.pad_or_truncate(logits, self.max_length)  # We pad or truncate the logits to have the same length for all sequences.\n",
    "\n",
    "        _, (hidden_state, _) = self.rnn(logits)  # We pass the logits to the LSTM layer and get the hidden state. The output of the LSTM is not used.\n",
    "\n",
    "        out = self.classifier(hidden_state.squeeze(0))  # We pass the hidden state to the Linear layer and get the output of the model.\n",
    "\n",
    "        return out  # We return the output of the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn-env",
   "language": "python",
   "name": "sklearn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
