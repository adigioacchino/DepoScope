{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Depo Detection tool\n",
    ">We finetuned the ESM2 model successfully (92% accuracy)<br>\n",
    ">The goal now is to stack a RNN layer for a binary classification into Dpo or Not Dpo categories\n",
    "***\n",
    "## I. Load prebuilt model \n",
    "## II. Stack RNN layer\n",
    "## III. Train Eval\n",
    "## IV. Metrics\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/concha-eloko/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at /media/concha-eloko/Linux/depolymerase_building/esm2_t12_35M_UR50D-finetuned-depolymerase/checkpoint-198/ were not used when initializing EsmForTokenClassification: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight']\n",
      "- This IS expected if you are initializing EsmForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer , AutoTokenizer\n",
    "\n",
    "import torch \n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n",
    "\n",
    "# Load the Prebuilt model :\n",
    "path_work = \"/home/conchae/PhageDepo_pdb\"\n",
    "model_path = f\"{path_work}/script_files/esm2_t30_150M_UR50D-finetuned-depolymerase.1608.3_labels.final/checkpoint-6015\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(df) :\n",
    "    labels_df = []\n",
    "    for _,row in df.iterrows():\n",
    "        info = row[\"Boundaries\"]\n",
    "        seq_length = len(row[\"Full_seq\"])\n",
    "        if info == \"Negative\" :\n",
    "            label = 0\n",
    "            labels_df.append(label)         \n",
    "        else :\n",
    "            label = 1\n",
    "            labels_df.append(label)\n",
    "    return labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the sequences :\n",
    "df_depo = pd.read_csv(f\"{path_work}/Phagedepo.Dataset.2007.tsv\" , sep = \"\\t\" , header = 0)\n",
    "\n",
    "df_beta_helix = df_depo[df_depo[\"Fold\"] == \"right-handed beta-helix\"]\n",
    "df_beta_prope = df_depo[df_depo[\"Fold\"] == \"6-bladed beta-propeller\"]\n",
    "df_beta_triple =  df_depo[df_depo[\"Fold\"] == \"triple-helix\"]\n",
    "df_negative = df_depo[df_depo[\"Fold\"] == \"Negative\"]\n",
    "\n",
    "# The phage proteins associated with PL16\n",
    "pl16_interpro = SeqIO.parse(f\"{path_work}/PL_16.phage_proteins.fasta\" , \"fasta\")\n",
    "seq_pl16_interpro = [str(record.seq) for record in pl16_interpro]\n",
    "labels_pl16 = [1]*len(seq_pl16_interpro)\n",
    "\n",
    "# Beta-helix :\n",
    "labels_beta_helix = get_labels(df_beta_helix)\n",
    "seq_beta_helix = df_beta_helix[\"Full_seq\"].to_list()\n",
    "\n",
    "# Beta propeller : \n",
    "labels_beta_propeller = get_labels(df_beta_prope)\n",
    "seq_beta_propeller = df_beta_prope[\"Full_seq\"].to_list()\n",
    "\n",
    "# Triple helix : \n",
    "labels_triple_helix = get_labels(df_beta_triple )\n",
    "seq_triple_helix = df_beta_triple[\"Full_seq\"].to_list()\n",
    "\n",
    "# Negative :\n",
    "labels_negative = get_labels(df_negative)\n",
    "seq_negative = df_negative[\"Full_seq\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input data :\n",
    "sequences = seq_beta_helix + seq_beta_propeller + seq_triple_helix + seq_negative\n",
    "labels = labels_beta_helix + labels_beta_propeller + labels_triple_helix + labels_negative\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.2, random_state = 243)\n",
    "train_esm2 , train_CNV , esm2_labels , CNV_labels = train_test_split(train_sequences, train_labels, test_size=0.25, random_state = 243)\n",
    "\n",
    "train_sequences_PL16, test_sequences_PL16, train_labels_PL16, test_labels_PL16 = train_test_split(seq_pl16_interpro, labels_pl16, test_size=0.5, random_state = 243)\n",
    "\n",
    "train_CNV = train_CNV + train_sequences_PL16\n",
    "CNV_labels = CNV_labels + train_labels_PL16\n",
    "\n",
    "test_sequences = test_sequences + test_sequences_PL16\n",
    "test_labels = test_labels + test_labels_PL16\n",
    "\n",
    "Dataset_train_df = pd.DataFrame({\"sequence\" : train_CNV , \"Label\" : CNV_labels})\n",
    "Dataset_test_df = pd.DataFrame({\"sequence\" : test_sequences , \"Label\" : test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************\n",
    "class Dpo_Dataset(Dataset):\n",
    "    def __init__(self, Dataset_df):\n",
    "        self.sequence = Dataset_df.sequence.values\n",
    "        self.labels = torch.tensor(Dataset_df[\"Label\"].values, dtype=torch.long)  # Subtract 1 if labels start from 1\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item_domain1 = self.sequence[idx]\n",
    "        item_domain2 = self.labels[idx]\n",
    "        return item_domain1, item_domain2\n",
    "\n",
    "train_singledata = Dpo_Dataset(Dataset_train_df)\n",
    "class Dpo_Dataset(Dataset):\n",
    "    def __init__(self, Dataset_df):\n",
    "        self.sequence = Dataset_df.sequence.values\n",
    "        self.labels = torch.tensor(Dataset_df[\"Label\"].values, dtype=torch.long)  # Subtract 1 if labels start from 1\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item_domain1 = self.sequence[idx]\n",
    "        item_domain2 = self.labels[idx]\n",
    "        return item_domain1, item_domain2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_singledata = Dpo_Dataset(Dataset_train_df)\n",
    "test_singledata = Dpo_Dataset(Dataset_test_df)\n",
    "\n",
    "train_single_loader = DataLoader(train_singledata, batch_size=12, shuffle=True, num_workers=4)\n",
    "test_single_loader = DataLoader(test_singledata, batch_size=12, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dpo_classifier(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(Dpo_classifier, self).__init__()\n",
    "        self.max_length = 1024\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=5, stride=1)  # Convolutional layer\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, stride=1)  # Convolutional layer\n",
    "        self.fc1 = nn.Linear(128 * (self.max_length - 2 * (5 - 1)), 32)  # calculate the output shape after 2 conv layers\n",
    "        self.classifier = nn.Linear(32, 1)  # Binary classification\n",
    "\n",
    "    def make_prediction(self, fasta_txt):\n",
    "        input_ids = tokenizer.encode(fasta_txt, truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            outputs = self.pretrained_model(input_ids)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            token_probs, token_ids = torch.max(probs, dim=-1)\n",
    "            tokens = token_ids.view(1, -1) # ensure 2D shape\n",
    "            return tokens\n",
    "\n",
    "    def pad_or_truncate(self, tokens):\n",
    "        if tokens.size(1) < self.max_length:\n",
    "            tokens = F.pad(tokens, (0, self.max_length - tokens.size(1)))\n",
    "        elif tokens.size(1) > self.max_length:\n",
    "            tokens = tokens[:, :self.max_length]\n",
    "        return tokens\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        batch_size = len(sequences)\n",
    "        tokens_batch = []\n",
    "        for seq in sequences:\n",
    "            tokens = self.make_prediction(seq)\n",
    "            tokens = self.pad_or_truncate(tokens)\n",
    "            tokens_batch.append(tokens)\n",
    "\n",
    "        outputs = torch.cat(tokens_batch).view(batch_size, 1, self.max_length)  # ensure 3D shape\n",
    "        outputs = outputs.float()  \n",
    "\n",
    "        out = F.relu(self.conv1(outputs))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = out.view(batch_size, -1)  # Flatten the tensor\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.classifier(out)\n",
    "        return out, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MALISQSIKNLKGGISQQPDILRYPDQGSRQVNGWSSETEGLQKRPPMVFIKTLGDRGALGQAPYIHLINRDENEQYYAVFTGNGIRVFDLAGNEKQVRYPNGSDYIKTSNPRNDLRMVTVADYTFVVNRNVAVQKNTTSVNLPNYNPKRDGLINVRGGQYGRELIVHINGKDVAKYKIPDGSQPAHVNNTDAQWLAEELAKQMRTNLSGWAVNVGQGFIHVAAPSGQQIDSFTTKDGYADQLINPVTHYAQSFSKLPPNAPNGYMVKVVGDASRSADQYYVRYDAERKVWVETLGWNTENQVRWETMPHALVRAADGNFDFKWLEWSPKSCGDIDTNPWPSFVGSSINDVFFFRNRLGFLSGENIILSRTAKYFNFYPASVANLSDDDPIDVAVSTNRISVLKYAVPFSEELLIWSDEAQFVLTASGTLTSKSVELNLTTQFDVQDRARPYGIGRNVYFASPRSSYTSIHRYYAVQDVSSVKNAEDITAHVPNYIPNGVFSICGSGTENFCSVLSHGDPSKIFMYKFLYLNEELRQQSWSHWDFGANVQVLACQSISSDMYVILRNEFNTFLTKISFTKNAIDLQGEPYRAFMDMKIRYTIPSGTYNDDTYNTSIHLPTIYGANFGRGRITVLEPDGKITVFEQPTAGWKSDPWLRLDGNLEGRMVYIGFNIDFVYEFSKFLIKQTADDGSSSTEDIGRLQLRRAWVNYENSGAFDIYVENQSSNWKYSMAGARLGSNTLRAGRLNLGTGQYRFPVVGNAKFNTVSILSDETTPLNIIGCGWEGNYLRRSSGI',\n",
       " tensor(1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model\n",
    "model_classifier = Dpo_classifier(model)\n",
    "model_classifier.train()\n",
    "\n",
    "optimizer = optim.Adam(model_classifier.parameters(), lr=0.001)  # Set learning rate\n",
    "criterion = nn.BCEWithLogitsLoss()  # Set loss function\n",
    "\n",
    "epochs = 10  # Number of training epochs\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    model_classifier.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    total_samples = 0\n",
    "    for i, (sequences, labels) in enumerate(train_single_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs, _ = model_classifier(sequences)\n",
    "        loss = criterion(outputs.view(-1), labels.float())  # Convert labels to float\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        predicted = (outputs > 0).float()  # Convert logits to predictions\n",
    "        # Comipute accuracy\n",
    "        #_, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        epoch_correct += (predicted == labels).sum().item()\n",
    "        # Accumulate loss\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {epoch_loss / len(train_single_loader):.4f}, Training Accuracy: {epoch_correct / total_samples:.4f}')\n",
    "    # Evaluation\n",
    "    model_classifier.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_single_loader:\n",
    "            outputs, _ = model_classifier(sequences)\n",
    "            #_, predicted = torch.max(outputs.data, 1)\n",
    "            predicted = (outputs > 0).float()\n",
    "            y_true.extend(labels.numpy())\n",
    "            y_pred.extend(predicted.numpy())            \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)  \n",
    "    recall = recall_score(y_true, y_pred)  \n",
    "    f1 = f1_score(y_true, y_pred)  \n",
    "    print(f'Testing Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finished Training')\n",
    "\n",
    "torch.save(model_classifier.state_dict(), f\"{path_work}/DepoDetection.esm2_t30_150M_UR50D.1608.final.model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geometric",
   "language": "python",
   "name": "torch_geometric"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
